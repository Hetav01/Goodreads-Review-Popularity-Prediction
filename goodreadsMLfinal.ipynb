{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.19.4)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.40.0-cp38-cp38-manylinux2010_x86_64.whl (571 kB)\n",
      "\u001b[K     |████████████████████████████████| 571 kB 80 kB/s eta 0:00:013\n",
      "\u001b[?25hRequirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.8/dist-packages (from shap) (4.56.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
      "\u001b[K     |███▉                            | 3.7 MB 23 kB/s eta 0:19:367^C\n",
      "\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.19.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 790 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "Successfully installed keras-2.9.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.8/dist-packages (0.13.2)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.8/dist-packages (from statsmodels) (0.5.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from statsmodels) (21.3)\n",
      "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.8/dist-packages (from statsmodels) (1.4.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.8/dist-packages (from statsmodels) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from statsmodels) (1.19.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=21.3->statsmodels) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25->statsmodels) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25->statsmodels) (2022.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5.2->statsmodels) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.6.1-py3-none-manylinux2014_x86_64.whl (192.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 192.9 MB 3.4 MB/s eta 0:00:01             | 2.6 MB 670 kB/s eta 0:04:4426 MB 4.7 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.19.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.6.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /usr/local/lib/python3.8/dist-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.19.4)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.8/dist-packages (from fasttext) (2.9.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (52.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (9.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (20.8)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.19.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gcld3 in /usr/local/lib/python3.8/dist-packages (3.0.13)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gcld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reviews:  15739967\n",
      "reviews with 1+ like/comment:  4706961\n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "error for text reviews count value:  \n",
      "total_books:  2360655\n",
      "books with 10+ reviews:  664632\n",
      "join reviews/books:  4048896\n",
      "reviews on books with 60+ review likes:  2354768\n",
      "pre english filter popularity count:\n",
      "popular\n",
      "0    1862572\n",
      "1     492196\n",
      "Name: user_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english reviews:  1981129\n",
      "final length:  1981129\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fasttext as ft\n",
    "\n",
    "review_path = \"goodreads_reviews_dedup.json\"\n",
    "book_path = \"goodreads_books.json\"\n",
    "\n",
    "# read review data, removing entries with 0 likes\n",
    "reviews = []\n",
    "total_reviews = 0\n",
    "with open(review_path) as f:\n",
    "    for line in f:\n",
    "        total_reviews += 1\n",
    "        entry = json.loads(line)\n",
    "        if entry['n_votes'] + entry['n_comments'] > 0:\n",
    "            reviews.append(entry)\n",
    "\n",
    "print(\"total_reviews: \", total_reviews)\n",
    "\n",
    "# convert to DF, retaining only relevant columns\n",
    "reviews = pd.DataFrame(reviews, columns=['user_id','book_id','rating','review_text','date_added','n_votes','n_comments'])\n",
    "print(\"reviews with 1+ like/comment: \", len(reviews))\n",
    "\n",
    "# calculate number of reviews per user\n",
    "user_reviews = reviews.groupby(\"user_id\")[\"book_id\"].count().rename(\"user_reviews\")\n",
    "\n",
    "# combine votes and comments\n",
    "reviews[\"review_likes\"] = reviews[\"n_votes\"] + reviews[\"n_comments\"]\n",
    "reviews = reviews.drop([\"n_votes\",\"n_comments\"],axis=1)\n",
    "\n",
    "# read book data, removing books with fewer than 10 reviews\n",
    "books = []\n",
    "total_books = 0\n",
    "with open(book_path) as f:\n",
    "    for line in f:\n",
    "        total_books += 1\n",
    "        entry = json.loads(line)\n",
    "        try:\n",
    "            if int(entry['text_reviews_count']) >= 10:\n",
    "                books.append(entry)\n",
    "        except:\n",
    "            print(\"error for text reviews count value: \", entry['text_reviews_count'])\n",
    "\n",
    "print(\"total_books: \", total_books)\n",
    "\n",
    "# convert to DF, retaining only relevant columns\n",
    "books = pd.DataFrame(books, columns=[\"book_id\",\"text_reviews_count\",\"ratings_count\",\"average_rating\"])\n",
    "print(\"books with 10+ reviews: \", len(books))\n",
    "\n",
    "# join reviews and books\n",
    "dat = pd.merge(reviews,books,on=\"book_id\")\n",
    "print(\"join reviews/books: \", len(dat))\n",
    "\n",
    "# calculate total review likes per book\n",
    "book_review_likes = dat.groupby(\"book_id\")[\"review_likes\"].sum().rename(\"book_review_likes\")\n",
    "\n",
    "# remove books with fewer than 60 total likes on reviews\n",
    "book_review_likes = book_review_likes[book_review_likes>=60]\n",
    "dat = dat.merge(book_review_likes,on='book_id')\n",
    "print(\"reviews on books with 60+ review likes: \", len(dat))\n",
    "\n",
    "# calculate like share\n",
    "dat[\"like_share\"] = dat[\"review_likes\"]/dat[\"book_review_likes\"]\n",
    "\n",
    "# create popularity binary variable\n",
    "popular_thresh = 0.02\n",
    "dat[\"popular\"] = np.where(dat[\"like_share\"]>popular_thresh,1,0)\n",
    "\n",
    "print(\"pre english filter popularity count:\")\n",
    "print(dat.groupby(\"popular\")[\"user_id\"].count())\n",
    "\n",
    "# filter to books in English\n",
    "language_model = ft.load_model('lid.176.bin')\n",
    "pred = language_model.predict(dat[\"review_text\"].str.replace('\\n','').to_list())\n",
    "keep_ind = [i for i in range(len(pred[0])) if pred[0][i][0] == '__label__en' and pred[1][i][0] > .90]\n",
    "dat = dat.iloc[keep_ind,].reset_index(drop=True)\n",
    "print(\"english reviews: \", len(dat))\n",
    "\n",
    "# create days since added column\n",
    "review_dates = pd.to_datetime(pd.to_datetime(dat[\"date_added\"],format='%a %b %d %H:%M:%S %z %Y',errors='coerce'),utc=True,errors='coerce')\n",
    "dat[\"days_since_review\"] = (max(review_dates) - review_dates).dt.days\n",
    "\n",
    "# add user_reviews column\n",
    "dat = dat.merge(user_reviews,on='user_id')\n",
    "\n",
    "# rename and reorder columns\n",
    "dat = dat.rename(columns={\"rating\": \"user_rating\", \"text_reviews_count\":\"book_reviews\", \"average_rating\":\"avg_rating\"})\n",
    "dat = dat[[\"book_id\", \"user_reviews\", \"user_rating\", \"avg_rating\", \"ratings_count\", \"review_text\", \"days_since_review\", \"review_likes\", \"like_share\", \"popular\"]]\n",
    "print(\"final length: \", len(dat))\n",
    "\n",
    "# save dataset\n",
    "dat.to_csv(\"filtered_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download(['punkt','averaged_perceptron_tagger','vader_lexicon','stopwords','wordnet'])\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# read review data\n",
    "dat = pd.read_csv(\"filtered_reviews.csv\")\n",
    "dat = dat.drop(columns=['book_id','ratings_count','review_likes','like_share'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_reviews  user_rating  avg_rating  \\\n",
      "0                 218            5        4.01   \n",
      "1                 218            5        3.90   \n",
      "2                 218            4        4.00   \n",
      "3                 218            3        4.10   \n",
      "4                 218            4        4.04   \n",
      "...               ...          ...         ...   \n",
      "1981124             1            5        3.86   \n",
      "1981125             4            5        3.40   \n",
      "1981126             1            4        3.71   \n",
      "1981127             5            3        4.26   \n",
      "1981128             1            5        3.62   \n",
      "\n",
      "                                               review_text  days_since_review  \\\n",
      "0        This is a special book. It started slow for ab...                 96   \n",
      "1        I decided to give up eating processed sugar fo...                298   \n",
      "2        Kevin Kelly, who is a Wired co-founder, lays o...                326   \n",
      "3        A fun, fast paced science fiction thriller. I ...                353   \n",
      "4        A fascinating book about community and belongi...                406   \n",
      "...                                                    ...                ...   \n",
      "1981124  This is Superb!!!! Amazingly well done!! I nev...               2614   \n",
      "1981125  This is a great book the advises are straight ...                779   \n",
      "1981126  What a blast. Perfect summer read.....fun, fas...               1738   \n",
      "1981127  While I enjoyed Masters, it lacked in some are...               1208   \n",
      "1981128  Probably the greatest book ever written, by bo...               3730   \n",
      "\n",
      "         popular  \n",
      "0              1  \n",
      "1              1  \n",
      "2              1  \n",
      "3              1  \n",
      "4              1  \n",
      "...          ...  \n",
      "1981124        1  \n",
      "1981125        0  \n",
      "1981126        0  \n",
      "1981127        0  \n",
      "1981128        1  \n",
      "\n",
      "[1981129 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_reviews  user_rating  days_since_review  popular  rating_diff  \\\n",
      "0                 218            5                 96        1         0.99   \n",
      "1                 218            5                298        1         1.10   \n",
      "2                 218            4                326        1         0.00   \n",
      "3                 218            3                353        1        -1.10   \n",
      "4                 218            4                406        1        -0.04   \n",
      "...               ...          ...                ...      ...          ...   \n",
      "1981124             1            5               2614        1         1.14   \n",
      "1981125             4            5                779        0         1.60   \n",
      "1981126             1            4               1738        0         0.29   \n",
      "1981127             5            3               1208        0        -1.26   \n",
      "1981128             1            5               3730        1         1.38   \n",
      "\n",
      "         quote                                    tokenized_sents  \\\n",
      "0         True  [This is a special book., It started slow for ...   \n",
      "1         True  [I decided to give up eating processed sugar f...   \n",
      "2         True  [Kevin Kelly, who is a Wired co-founder, lays ...   \n",
      "3         True  [A fun, fast paced science fiction thriller., ...   \n",
      "4         True  [A fascinating book about community and belong...   \n",
      "...        ...                                                ...   \n",
      "1981124  False  [This is Superb!!!!, Amazingly well done!!, I ...   \n",
      "1981125  False  [This is a great book the advises are straight...   \n",
      "1981126  False  [What a blast., Perfect summer read.....fun, f...   \n",
      "1981127  False  [While I enjoyed Masters, it lacked in some ar...   \n",
      "1981128   True  [Probably the greatest book ever written, by b...   \n",
      "\n",
      "                                           tokenized_words  num_words  \\\n",
      "0        [this, is, a, special, book, it, started, slow...        354   \n",
      "1        [i, decided, to, give, up, eating, processed, ...        966   \n",
      "2        [kevin, kelly, who, is, a, wired, lays, out, t...        567   \n",
      "3        [a, fun, fast, paced, science, fiction, thrill...        458   \n",
      "4        [a, fascinating, book, about, community, and, ...        494   \n",
      "...                                                    ...        ...   \n",
      "1981124  [this, is, superb, amazingly, well, done, i, n...         60   \n",
      "1981125  [this, is, a, great, book, the, advises, are, ...         25   \n",
      "1981126  [what, a, blast, perfect, summer, read, fashio...         21   \n",
      "1981127  [while, i, enjoyed, masters, it, lacked, in, s...        264   \n",
      "1981128  [probably, the, greatest, book, ever, written,...        471   \n",
      "\n",
      "         avg_sent_len  avg_word_len  \n",
      "0           17.700000      4.618644  \n",
      "1           21.466667      4.623188  \n",
      "2           20.250000      4.414462  \n",
      "3           13.085714      4.303493  \n",
      "4           27.444444      4.757085  \n",
      "...               ...           ...  \n",
      "1981124      8.571429      4.766667  \n",
      "1981125     12.500000      4.800000  \n",
      "1981126      5.250000      4.142857  \n",
      "1981127     12.000000      4.083333  \n",
      "1981128     27.705882      4.416136  \n",
      "\n",
      "[1981129 rows x 11 columns]\n",
      "         user_reviews  user_rating  days_since_review  popular  rating_diff  \\\n",
      "0                 218            5                 96        1         0.99   \n",
      "1                 218            5                298        1         1.10   \n",
      "2                 218            4                326        1         0.00   \n",
      "3                 218            3                353        1        -1.10   \n",
      "4                 218            4                406        1        -0.04   \n",
      "...               ...          ...                ...      ...          ...   \n",
      "1981124             1            5               2614        1         1.14   \n",
      "1981125             4            5                779        0         1.60   \n",
      "1981126             1            4               1738        0         0.29   \n",
      "1981127             5            3               1208        0        -1.26   \n",
      "1981128             1            5               3730        1         1.38   \n",
      "\n",
      "         quote                                    tokenized_sents  \\\n",
      "0         True  [This is a special book., It started slow for ...   \n",
      "1         True  [I decided to give up eating processed sugar f...   \n",
      "2         True  [Kevin Kelly, who is a Wired co-founder, lays ...   \n",
      "3         True  [A fun, fast paced science fiction thriller., ...   \n",
      "4         True  [A fascinating book about community and belong...   \n",
      "...        ...                                                ...   \n",
      "1981124  False  [This is Superb!!!!, Amazingly well done!!, I ...   \n",
      "1981125  False  [This is a great book the advises are straight...   \n",
      "1981126  False  [What a blast., Perfect summer read.....fun, f...   \n",
      "1981127  False  [While I enjoyed Masters, it lacked in some ar...   \n",
      "1981128   True  [Probably the greatest book ever written, by b...   \n",
      "\n",
      "                                           tokenized_words  num_words  \\\n",
      "0        [this, is, a, special, book, it, started, slow...        354   \n",
      "1        [i, decided, to, give, up, eating, processed, ...        966   \n",
      "2        [kevin, kelly, who, is, a, wired, lays, out, t...        567   \n",
      "3        [a, fun, fast, paced, science, fiction, thrill...        458   \n",
      "4        [a, fascinating, book, about, community, and, ...        494   \n",
      "...                                                    ...        ...   \n",
      "1981124  [this, is, superb, amazingly, well, done, i, n...         60   \n",
      "1981125  [this, is, a, great, book, the, advises, are, ...         25   \n",
      "1981126  [what, a, blast, perfect, summer, read, fashio...         21   \n",
      "1981127  [while, i, enjoyed, masters, it, lacked, in, s...        264   \n",
      "1981128  [probably, the, greatest, book, ever, written,...        471   \n",
      "\n",
      "         avg_sent_len  avg_word_len  pct_verbs  pct_nouns   pct_adj  \n",
      "0           17.700000      4.618644   0.192090   0.225989  0.163842  \n",
      "1           21.466667      4.623188   0.180124   0.236025  0.187371  \n",
      "2           20.250000      4.414462   0.186949   0.222222  0.185185  \n",
      "3           13.085714      4.303493   0.209607   0.235808  0.120087  \n",
      "4           27.444444      4.757085   0.155870   0.275304  0.147773  \n",
      "...               ...           ...        ...        ...       ...  \n",
      "1981124      8.571429      4.766667   0.200000   0.216667  0.233333  \n",
      "1981125     12.500000      4.800000   0.280000   0.080000  0.280000  \n",
      "1981126      5.250000      4.142857   0.142857   0.285714  0.142857  \n",
      "1981127     12.000000      4.083333   0.227273   0.219697  0.155303  \n",
      "1981128     27.705882      4.416136   0.144374   0.290870  0.154989  \n",
      "\n",
      "[1981129 rows x 14 columns]\n",
      "         user_reviews  user_rating  days_since_review  popular  rating_diff  \\\n",
      "0                 218            5                 96        1         0.99   \n",
      "1                 218            5                298        1         1.10   \n",
      "2                 218            4                326        1         0.00   \n",
      "3                 218            3                353        1        -1.10   \n",
      "4                 218            4                406        1        -0.04   \n",
      "...               ...          ...                ...      ...          ...   \n",
      "1981124             1            5               2614        1         1.14   \n",
      "1981125             4            5                779        0         1.60   \n",
      "1981126             1            4               1738        0         0.29   \n",
      "1981127             5            3               1208        0        -1.26   \n",
      "1981128             1            5               3730        1         1.38   \n",
      "\n",
      "         quote                                    tokenized_words  num_words  \\\n",
      "0         True  [this, is, a, special, book, it, started, slow...        354   \n",
      "1         True  [i, decided, to, give, up, eating, processed, ...        966   \n",
      "2         True  [kevin, kelly, who, is, a, wired, lays, out, t...        567   \n",
      "3         True  [a, fun, fast, paced, science, fiction, thrill...        458   \n",
      "4         True  [a, fascinating, book, about, community, and, ...        494   \n",
      "...        ...                                                ...        ...   \n",
      "1981124  False  [this, is, superb, amazingly, well, done, i, n...         60   \n",
      "1981125  False  [this, is, a, great, book, the, advises, are, ...         25   \n",
      "1981126  False  [what, a, blast, perfect, summer, read, fashio...         21   \n",
      "1981127  False  [while, i, enjoyed, masters, it, lacked, in, s...        264   \n",
      "1981128   True  [probably, the, greatest, book, ever, written,...        471   \n",
      "\n",
      "         avg_sent_len  avg_word_len  pct_verbs  pct_nouns   pct_adj  sentiment  \n",
      "0           17.700000      4.618644   0.192090   0.225989  0.163842   0.131265  \n",
      "1           21.466667      4.623188   0.180124   0.236025  0.187371   0.030033  \n",
      "2           20.250000      4.414462   0.186949   0.222222  0.185185   0.227825  \n",
      "3           13.085714      4.303493   0.209607   0.235808  0.120087   0.099969  \n",
      "4           27.444444      4.757085   0.155870   0.275304  0.147773   0.047972  \n",
      "...               ...           ...        ...        ...       ...        ...  \n",
      "1981124      8.571429      4.766667   0.200000   0.216667  0.233333   0.074271  \n",
      "1981125     12.500000      4.800000   0.280000   0.080000  0.280000   0.816900  \n",
      "1981126      5.250000      4.142857   0.142857   0.285714  0.142857   0.159750  \n",
      "1981127     12.000000      4.083333   0.227273   0.219697  0.155303   0.242600  \n",
      "1981128     27.705882      4.416136   0.144374   0.290870  0.154989  -0.020624  \n",
      "\n",
      "[1981129 rows x 14 columns]\n",
      "         popular  user_reviews  days_since_review  user_rating  rating_diff  \\\n",
      "0              1           218                 96            5         0.99   \n",
      "1              1           218                298            5         1.10   \n",
      "2              1           218                326            4         0.00   \n",
      "3              1           218                353            3        -1.10   \n",
      "4              1           218                406            4        -0.04   \n",
      "...          ...           ...                ...          ...          ...   \n",
      "1981124        1             1               2614            5         1.14   \n",
      "1981125        0             4                779            5         1.60   \n",
      "1981126        0             1               1738            4         0.29   \n",
      "1981127        0             5               1208            3        -1.26   \n",
      "1981128        1             1               3730            5         1.38   \n",
      "\n",
      "         num_words  avg_word_len  avg_sent_len  pct_verbs  pct_nouns  \\\n",
      "0              354      4.618644     17.700000   0.192090   0.225989   \n",
      "1              966      4.623188     21.466667   0.180124   0.236025   \n",
      "2              567      4.414462     20.250000   0.186949   0.222222   \n",
      "3              458      4.303493     13.085714   0.209607   0.235808   \n",
      "4              494      4.757085     27.444444   0.155870   0.275304   \n",
      "...            ...           ...           ...        ...        ...   \n",
      "1981124         60      4.766667      8.571429   0.200000   0.216667   \n",
      "1981125         25      4.800000     12.500000   0.280000   0.080000   \n",
      "1981126         21      4.142857      5.250000   0.142857   0.285714   \n",
      "1981127        264      4.083333     12.000000   0.227273   0.219697   \n",
      "1981128        471      4.416136     27.705882   0.144374   0.290870   \n",
      "\n",
      "          pct_adj  quote  sentiment  \\\n",
      "0        0.163842   True   0.131265   \n",
      "1        0.187371   True   0.030033   \n",
      "2        0.185185   True   0.227825   \n",
      "3        0.120087   True   0.099969   \n",
      "4        0.147773   True   0.047972   \n",
      "...           ...    ...        ...   \n",
      "1981124  0.233333  False   0.074271   \n",
      "1981125  0.280000  False   0.816900   \n",
      "1981126  0.142857  False   0.159750   \n",
      "1981127  0.155303  False   0.242600   \n",
      "1981128  0.154989   True  -0.020624   \n",
      "\n",
      "                                           tokenized_words  \n",
      "0        [special, book, started, slow, first, third, m...  \n",
      "1        [decided, give, eating, processed, sugar, mont...  \n",
      "2        [kevin, kelly, wired, lay, technological, tren...  \n",
      "3        [fun, fast, paced, science, fiction, thriller,...  \n",
      "4        [fascinating, book, community, belonging, mode...  \n",
      "...                                                    ...  \n",
      "1981124  [superb, amazingly, well, done, never, realize...  \n",
      "1981125  [great, book, advises, straight, forward, enco...  \n",
      "1981126  [blast, perfect, summer, read, fashion, top, f...  \n",
      "1981127  [enjoyed, master, lacked, area, pro, sex, hot,...  \n",
      "1981128  [probably, greatest, book, ever, written, man,...  \n",
      "\n",
      "[1981129 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# difference between user rating and average book rating\n",
    "dat[\"rating_diff\"] = dat[\"user_rating\"]-dat[\"avg_rating\"]\n",
    "dat = dat.drop(columns=['avg_rating'])\n",
    "\n",
    "# flag if review contains a quotation\n",
    "dat[\"quote\"] = dat[\"review_text\"].str.contains(\"\\\"\")\n",
    "\n",
    "# tokenize for review length (num words), avg sentence length, avg word length\n",
    "dat[\"tokenized_sents\"] = dat[\"review_text\"].apply(nltk.tokenize.sent_tokenize)\n",
    "dat[\"num_sentences\"] = dat[\"tokenized_sents\"].apply(len)\n",
    "\n",
    "dat[\"tokenized_words\"] = dat[\"review_text\"].apply(lambda review: [word.lower() for word in nltk.tokenize.word_tokenize(review) if word.isalpha()])\n",
    "dat[\"num_words\"] = dat[\"tokenized_words\"].apply(len)\n",
    "dat[\"avg_sent_len\"] = dat[\"num_words\"]/dat[\"num_sentences\"]\n",
    "dat[\"num_letters\"] = dat[\"tokenized_words\"].apply(lambda review: len([letter for word in review for letter in word]))\n",
    "dat[\"avg_word_len\"] = dat[\"num_letters\"]/dat[\"num_words\"]\n",
    "dat = dat.drop(columns=['review_text','num_sentences','num_letters'])\n",
    "\n",
    "print(dat)\n",
    "\n",
    "# part of speech tagging\n",
    "dat[\"pos_tags\"] = dat[\"tokenized_words\"].apply(nltk.pos_tag)\n",
    "\n",
    "def count_pos(pos_tags, pos):\n",
    "    counts = 0\n",
    "    for word in pos_tags:\n",
    "        if word[1][0] in pos:\n",
    "            counts += 1\n",
    "    return(counts)\n",
    "\n",
    "dat[\"verbs\"] = dat[\"pos_tags\"].apply(count_pos,pos=[\"V\"])\n",
    "dat[\"pct_verbs\"] = dat[\"verbs\"]/dat[\"num_words\"]\n",
    "dat[\"nouns\"] = dat[\"pos_tags\"].apply(count_pos,pos=[\"N\"])\n",
    "dat[\"pct_nouns\"] = dat[\"nouns\"]/dat[\"num_words\"]\n",
    "dat[\"adj\"] = dat[\"pos_tags\"].apply(count_pos,pos=[\"J\",\"R\"])\n",
    "dat[\"pct_adj\"] = dat[\"adj\"]/dat[\"num_words\"]\n",
    "dat = dat.drop(columns=['pos_tags','verbs','nouns','adj'])\n",
    "\n",
    "print(dat)\n",
    "\n",
    "# sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def review_sentiment(review_sents):\n",
    "    comptot = 0\n",
    "    for sentence in review_sents:\n",
    "        scores = sid.polarity_scores(sentence)\n",
    "        comptot += scores['compound']\n",
    "    return(comptot/len(review_sents))\n",
    "\n",
    "dat[\"sentiment\"] = dat[\"tokenized_sents\"].apply(review_sentiment)\n",
    "dat = dat.drop(columns=['tokenized_sents'])\n",
    "\n",
    "print(dat)\n",
    "# further text processing\n",
    "# remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda review: [word for word in review if word not in stopwords])\n",
    "# lemmatization\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda review: [wnl.lemmatize(word) for word in review])\n",
    "\n",
    "dat = dat[[\"popular\",\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\",\"tokenized_words\"]]\n",
    "\n",
    "print(dat)\n",
    "# save dataset\n",
    "dat.to_csv(\"tokenized_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION SUBSET A\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.478501\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                popular   No. Observations:              1683881\n",
      "Model:                          Logit   Df Residuals:                  1683876\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Sun, 15 May 2022   Pseudo R-squ.:                 0.05992\n",
      "Time:                        17:26:13   Log-Likelihood:            -8.0574e+05\n",
      "converged:                       True   LL-Null:                   -8.5710e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 1.1043      0.028     39.828      0.000       1.050       1.159\n",
      "user_reviews          0.0013   4.77e-06    276.312      0.000       0.001       0.001\n",
      "days_since_review -6.408e-05   2.37e-06    -26.998      0.000   -6.87e-05   -5.94e-05\n",
      "user_rating          -0.6911      0.007   -100.495      0.000      -0.705      -0.678\n",
      "rating_diff           0.6986      0.007     97.997      0.000       0.685       0.713\n",
      "=====================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88    235680\n",
      "           1       0.55      0.08      0.14     61476\n",
      "\n",
      "    accuracy                           0.80    297156\n",
      "   macro avg       0.67      0.53      0.51    297156\n",
      "weighted avg       0.75      0.80      0.73    297156\n",
      "\n",
      "[[231499   4181]\n",
      " [ 56467   5009]]\n",
      "0.5318693974957391\n",
      "\n",
      "\n",
      "LOGISTIC REGRESSION SUBSET B\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.466191\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                popular   No. Observations:              1683881\n",
      "Model:                          Logit   Df Residuals:                  1683868\n",
      "Method:                           MLE   Df Model:                           12\n",
      "Date:                Sun, 15 May 2022   Pseudo R-squ.:                 0.08411\n",
      "Time:                        17:26:23   Log-Likelihood:            -7.8501e+05\n",
      "converged:                       True   LL-Null:                   -8.5710e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 0.4590      0.042     10.879      0.000       0.376       0.542\n",
      "user_reviews          0.0012   4.75e-06    246.452      0.000       0.001       0.001\n",
      "days_since_review -7.564e-05   2.42e-06    -31.320      0.000   -8.04e-05   -7.09e-05\n",
      "user_rating          -0.6444      0.007    -91.699      0.000      -0.658      -0.631\n",
      "rating_diff           0.6425      0.007     88.459      0.000       0.628       0.657\n",
      "num_words             0.0012   8.93e-06    138.964      0.000       0.001       0.001\n",
      "avg_word_len          0.1693      0.006     29.866      0.000       0.158       0.180\n",
      "avg_sent_len          0.0035      0.000     13.365      0.000       0.003       0.004\n",
      "pct_verbs            -1.8653      0.054    -34.599      0.000      -1.971      -1.760\n",
      "pct_nouns             0.1045      0.047      2.207      0.027       0.012       0.197\n",
      "pct_adj              -1.3026      0.044    -29.645      0.000      -1.389      -1.216\n",
      "quote                 0.1630      0.005     34.034      0.000       0.154       0.172\n",
      "sentiment            -0.1169      0.009    -12.350      0.000      -0.135      -0.098\n",
      "=====================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.88    235680\n",
      "           1       0.56      0.11      0.18     61476\n",
      "\n",
      "    accuracy                           0.80    297156\n",
      "   macro avg       0.68      0.54      0.53    297156\n",
      "weighted avg       0.76      0.80      0.74    297156\n",
      "\n",
      "[[230560   5120]\n",
      " [ 54899   6577]]\n",
      "0.5426302337911677\n",
      "\n",
      "\n",
      "UNDERSAMPLE\n",
      "LOGISTIC REGRESSION SUBSET A\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.644113\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                popular   No. Observations:               694738\n",
      "Model:                          Logit   Df Residuals:                   694733\n",
      "Method:                           MLE   Df Model:                            4\n",
      "Date:                Sun, 15 May 2022   Pseudo R-squ.:                 0.07074\n",
      "Time:                        17:26:26   Log-Likelihood:            -4.4749e+05\n",
      "converged:                       True   LL-Null:                   -4.8156e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 2.3475      0.036     65.136      0.000       2.277       2.418\n",
      "user_reviews          0.0016   8.06e-06    204.359      0.000       0.002       0.002\n",
      "days_since_review -5.371e-05   2.97e-06    -18.084      0.000   -5.95e-05   -4.79e-05\n",
      "user_rating          -0.6925      0.009    -77.980      0.000      -0.710      -0.675\n",
      "rating_diff           0.7012      0.009     76.364      0.000       0.683       0.719\n",
      "=====================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82    235680\n",
      "           1       0.38      0.51      0.43     61476\n",
      "\n",
      "    accuracy                           0.73    297156\n",
      "   macro avg       0.62      0.64      0.63    297156\n",
      "weighted avg       0.76      0.73      0.74    297156\n",
      "\n",
      "[[184518  51162]\n",
      " [ 30366  31110]]\n",
      "0.6444843285919933\n",
      "\n",
      "\n",
      "LOGISTIC REGRESSION SUBSET B\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.626614\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                popular   No. Observations:               694738\n",
      "Model:                          Logit   Df Residuals:                   694725\n",
      "Method:                           MLE   Df Model:                           12\n",
      "Date:                Sun, 15 May 2022   Pseudo R-squ.:                 0.09599\n",
      "Time:                        17:26:29   Log-Likelihood:            -4.3533e+05\n",
      "converged:                       True   LL-Null:                   -4.8156e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 1.5967      0.054     29.415      0.000       1.490       1.703\n",
      "user_reviews          0.0014      8e-06    180.496      0.000       0.001       0.001\n",
      "days_since_review  -6.98e-05   3.03e-06    -23.052      0.000   -7.57e-05   -6.39e-05\n",
      "user_rating          -0.6428      0.009    -70.708      0.000      -0.661      -0.625\n",
      "rating_diff           0.6398      0.009     68.204      0.000       0.621       0.658\n",
      "num_words             0.0013   1.35e-05     99.294      0.000       0.001       0.001\n",
      "avg_word_len          0.1806      0.007     25.079      0.000       0.167       0.195\n",
      "avg_sent_len          0.0033      0.000      9.328      0.000       0.003       0.004\n",
      "pct_verbs            -1.6797      0.067    -25.115      0.000      -1.811      -1.549\n",
      "pct_nouns             0.1281      0.061      2.108      0.035       0.009       0.247\n",
      "pct_adj              -1.2993      0.054    -23.841      0.000      -1.406      -1.192\n",
      "quote                 0.1563      0.006     24.919      0.000       0.144       0.169\n",
      "sentiment            -0.0816      0.012     -6.826      0.000      -0.105      -0.058\n",
      "=====================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.74      0.80    235680\n",
      "           1       0.37      0.57      0.45     61476\n",
      "\n",
      "    accuracy                           0.71    297156\n",
      "   macro avg       0.62      0.66      0.62    297156\n",
      "weighted avg       0.77      0.71      0.73    297156\n",
      "\n",
      "[[175524  60156]\n",
      " [ 26457  35019]]\n",
      "0.6571962661500609\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# feature subsets\n",
    "subset_a = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\"]\n",
    "subset_b = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\n",
    "            \"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\n",
    "            \"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\"]\n",
    "\n",
    "# SUBSET A\n",
    "print(\"LOGISTIC REGRESSION SUBSET A\")\n",
    "\n",
    "# train\n",
    "log_reg = sm.Logit(y_train, sm.add_constant(X_train[subset_a])).fit()\n",
    "print(log_reg.summary())\n",
    "\n",
    "# predict\n",
    "predictions = log_reg.predict(sm.add_constant(X_test[subset_a]))\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nLOGISTIC REGRESSION SUBSET B\")\n",
    "\n",
    "# train\n",
    "log_reg = sm.Logit(y_train, sm.add_constant(X_train[subset_b])).fit()\n",
    "print(log_reg.summary())\n",
    "\n",
    "# predict\n",
    "predictions = log_reg.predict(sm.add_constant(X_test[subset_b]))\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# UNDERSAMPLE\n",
    "print(\"\\n\\nUNDERSAMPLE\")\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "# SUBSET A\n",
    "print(\"LOGISTIC REGRESSION SUBSET A\")\n",
    "\n",
    "# train\n",
    "log_reg = sm.Logit(y_train, sm.add_constant(X_train[subset_a])).fit()\n",
    "print(log_reg.summary())\n",
    "\n",
    "# predict\n",
    "predictions = log_reg.predict(sm.add_constant(X_test[subset_a]))\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nLOGISTIC REGRESSION SUBSET B\")\n",
    "\n",
    "# train\n",
    "log_reg = sm.Logit(y_train, sm.add_constant(X_train[subset_b])).fit()\n",
    "print(log_reg.summary())\n",
    "\n",
    "# predict\n",
    "predictions = log_reg.predict(sm.add_constant(X_test[subset_b]))\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "# BAG OF WORDS\n",
    "print(\"\\n\\nLOGISTIC REGRESSION BOW\")\n",
    "\n",
    "# pipeline\n",
    "bow_pipe = make_pipeline(\n",
    "    ColumnTransformer(remainder='passthrough',\n",
    "                      transformers=[('countvectorizer',\n",
    "                                     CountVectorizer(),\n",
    "                                     'tokenized_words')]),\n",
    "    StandardScaler(with_mean=False),\n",
    "    LogisticRegression(penalty='l2',\n",
    "                       solver='saga',\n",
    "                       max_iter=1000,\n",
    "                       random_state=229,\n",
    "                       n_jobs=-1))\n",
    "\n",
    "# parameters to try\n",
    "parameters = {\n",
    "    'columntransformer__countvectorizer__max_features': (10000,50000),\n",
    "    'logisticregression__C': (10, 1, 0.01, 0.001)\n",
    "}\n",
    "\n",
    "# perform validation\n",
    "gs_bow_pipe = GridSearchCV(bow_pipe, \n",
    "                           parameters, \n",
    "                           cv=ShuffleSplit(n_splits=1, \n",
    "                                           test_size=0.13, \n",
    "                                           random_state=229))\n",
    "gs_bow_pipe.fit(X_train, y_train)\n",
    "print(gs_bow_pipe.cv_results_)\n",
    "print(gs_bow_pipe.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_bow_pipe.predict(X_test)\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "coefficients = gs_bow_pipe.best_estimator_.named_steps['logisticregression'].coef_[0]\n",
    "num_nonzero_coefs = len(np.where(abs(coefficients) > 0)[0])\n",
    "sorted_ind = np.argsort(abs(coefficients))[::-1][:num_nonzero_coefs]\n",
    "print(len(sorted_ind))\n",
    "print(np.take(coefficients,sorted_ind.tolist()))\n",
    "print(np.take(gs_bow_pipe.best_estimator_.named_steps['columntransformer'].get_feature_names(),sorted_ind.tolist()))\n",
    "\n",
    "# TF-IDF\n",
    "print(\"\\n\\nLOGISTIC REGRESSION TF-IDF\")\n",
    "\n",
    "# pipeline\n",
    "tf_pipe = make_pipeline(\n",
    "    ColumnTransformer(remainder='passthrough',\n",
    "                      transformers=[('tfidfvectorizer',\n",
    "                                     TfidfVectorizer(),\n",
    "                                     'tokenized_words')]),\n",
    "    StandardScaler(with_mean=False),\n",
    "    LogisticRegression(penalty='l2',\n",
    "                       solver='saga',\n",
    "                       max_iter=1000,\n",
    "                       random_state=229,\n",
    "                       n_jobs=-1))\n",
    "\n",
    "# parameters to try\n",
    "parameters = {\n",
    "    'logisticregression__C': (10, 1, 0.01, 0.001)\n",
    "}\n",
    "\n",
    "# perform validation\n",
    "gs_tf_pipe = GridSearchCV(tf_pipe, \n",
    "                           parameters, \n",
    "                           cv=ShuffleSplit(n_splits=1, \n",
    "                                           test_size=0.13, \n",
    "                                           random_state=229))\n",
    "gs_tf_pipe.fit(X_train, y_train)\n",
    "print(gs_tf_pipe.cv_results_)\n",
    "print(gs_tf_pipe.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_tf_pipe.predict(X_test)\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "coefficients = gs_tf_pipe.best_estimator_.named_steps['logisticregression'].coef_[0]\n",
    "num_nonzero_coefs = len(np.where(abs(coefficients) > 0)[0])\n",
    "sorted_ind = np.argsort(abs(coefficients))[::-1][:num_nonzero_coefs]\n",
    "print(len(sorted_ind))\n",
    "print(np.take(coefficients,sorted_ind.tolist()))\n",
    "print(np.take(gs_tf_pipe.best_estimator_.named_steps['columntransformer'].get_feature_names(),sorted_ind.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST SUBSET A\n",
      "{'mean_fit_time': array([  6.17518568,  11.65195394, 112.09067631,  11.47117496,\n",
      "        22.5085535 , 221.82397866,  17.90258598,  35.75986719,\n",
      "       350.60607505,   6.03085041,  11.72681546, 111.35820556,\n",
      "        11.6582942 ,  22.43667459, 218.423352  ,  18.48060012,\n",
      "        35.77954102, 339.26907277,   6.2518177 ,  11.59589577,\n",
      "       111.88311243,  11.50949597,  22.40764499, 219.68212318,\n",
      "        17.93870211,  34.7679317 , 340.38827324]), 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'mean_score_time': array([0.03138947, 0.02668715, 0.06223559, 0.0279634 , 0.03393793,\n",
      "       0.11465549, 0.03015161, 0.03846765, 0.18306708, 0.01909733,\n",
      "       0.02707171, 0.0619247 , 0.02649117, 0.03301668, 0.13114214,\n",
      "       0.03258967, 0.03867865, 0.17232108, 0.02676797, 0.02956104,\n",
      "       0.06817889, 0.03263092, 0.03188157, 0.1246593 , 0.03151417,\n",
      "       0.03668737, 0.17434812]), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 2, 2, 2, 4, 4, 4, 6, 6, 6,\n",
      "                   2, 2, 2, 4, 4, 4, 6, 6, 6],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50, 100,\n",
      "                   1000, 50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50,\n",
      "                   100, 1000, 50, 100, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 1000}], 'split0_test_score': array([-0.13340029,  0.01110356,  0.1096891 , -0.12769401,  0.01875676,\n",
      "        0.12392613, -0.12289502,  0.02603868,  0.13850663,  0.10502355,\n",
      "        0.10962187,  0.13707677,  0.11396862,  0.12418291,  0.15943074,\n",
      "        0.12744974,  0.13862886,  0.17282477,  0.11357525,  0.1223289 ,\n",
      "        0.15079415,  0.1307001 ,  0.14068661,  0.17260603,  0.14760195,\n",
      "        0.15854099,  0.17769028]), 'mean_test_score': array([-0.13340029,  0.01110356,  0.1096891 , -0.12769401,  0.01875676,\n",
      "        0.12392613, -0.12289502,  0.02603868,  0.13850663,  0.10502355,\n",
      "        0.10962187,  0.13707677,  0.11396862,  0.12418291,  0.15943074,\n",
      "        0.12744974,  0.13862886,  0.17282477,  0.11357525,  0.1223289 ,\n",
      "        0.15079415,  0.1307001 ,  0.14068661,  0.17260603,  0.14760195,\n",
      "        0.15854099,  0.17769028]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([27, 24, 19, 26, 23, 15, 25, 22, 10, 21, 20, 11, 17, 14,  4, 13,  9,\n",
      "        2, 18, 16,  6, 12,  8,  3,  7,  5,  1], dtype=int32)}\n",
      "{'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 1000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90    235680\n",
      "           1       0.71      0.23      0.35     61476\n",
      "\n",
      "    accuracy                           0.82    297156\n",
      "   macro avg       0.77      0.60      0.62    297156\n",
      "weighted avg       0.80      0.82      0.78    297156\n",
      "\n",
      "[[229839   5841]\n",
      " [ 47318  14158]]\n",
      "0.6027588254433137\n",
      "[0.50247324 0.1150267  0.278516   0.10398406]\n",
      "\n",
      "\n",
      "RANDOM FOREST SUBSET B\n",
      "{'mean_fit_time': array([  8.05342245,  15.64956689, 146.79102254,  15.59762907,\n",
      "        31.32783914, 310.07152724,  26.35233331,  52.21252108,\n",
      "       527.6687839 ,   8.16403413,  15.46900535, 146.24808145,\n",
      "        16.51630735,  32.2561667 , 304.10967612,  27.81272602,\n",
      "        54.10523748, 506.30365849,   8.25275707,  15.4528532 ,\n",
      "       146.52653909,  16.48817253,  31.31286263, 302.06202531,\n",
      "        27.1310792 ,  51.63676786, 506.30229402]), 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'mean_score_time': array([0.04119992, 0.05088067, 0.08323932, 0.04117608, 0.04968333,\n",
      "       0.13302398, 0.04324865, 0.05592418, 0.20544195, 0.03912091,\n",
      "       0.04194331, 0.08300591, 0.05268097, 0.05934954, 0.15808082,\n",
      "       0.04453182, 0.05149198, 0.17930818, 0.04783106, 0.03956604,\n",
      "       0.07790899, 0.0408926 , 0.04428267, 0.13008261, 0.04601455,\n",
      "       0.05531526, 0.20265675]), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 2, 2, 2, 4, 4, 4, 6, 6, 6,\n",
      "                   2, 2, 2, 4, 4, 4, 6, 6, 6],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50, 100,\n",
      "                   1000, 50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50,\n",
      "                   100, 1000, 50, 100, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 1000}], 'split0_test_score': array([-0.129636  ,  0.01906897,  0.13240732, -0.11827032,  0.0326981 ,\n",
      "        0.14391918, -0.1121212 ,  0.04162332,  0.15729358,  0.12467729,\n",
      "        0.13268415,  0.15755473,  0.13566743,  0.14419115,  0.17812757,\n",
      "        0.14680698,  0.15776947,  0.19146515,  0.1354909 ,  0.14318699,\n",
      "        0.17032558,  0.15083072,  0.16224997,  0.18933613,  0.1641249 ,\n",
      "        0.17448498,  0.19119215]), 'mean_test_score': array([-0.129636  ,  0.01906897,  0.13240732, -0.11827032,  0.0326981 ,\n",
      "        0.14391918, -0.1121212 ,  0.04162332,  0.15729358,  0.12467729,\n",
      "        0.13268415,  0.15755473,  0.13566743,  0.14419115,  0.17812757,\n",
      "        0.14680698,  0.15776947,  0.19146515,  0.1354909 ,  0.14318699,\n",
      "        0.17032558,  0.15083072,  0.16224997,  0.18933613,  0.1641249 ,\n",
      "        0.17448498,  0.19119215]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([27, 24, 20, 26, 23, 15, 25, 22, 11, 21, 19, 10, 17, 14,  4, 13,  9,\n",
      "        1, 18, 16,  6, 12,  8,  3,  7,  5,  2], dtype=int32)}\n",
      "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90    235680\n",
      "           1       0.73      0.22      0.34     61476\n",
      "\n",
      "    accuracy                           0.82    297156\n",
      "   macro avg       0.78      0.60      0.62    297156\n",
      "weighted avg       0.81      0.82      0.78    297156\n",
      "\n",
      "[[230640   5040]\n",
      " [ 47685  13791]]\n",
      "0.6014732588506051\n",
      "[0.31340075 0.0562859  0.14043407 0.05655252 0.18757644 0.03166446\n",
      " 0.02409366 0.02830949 0.03050884 0.01990081 0.09120139 0.02007171]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "#changed the test_size to 15%.\n",
    "# feature subsets\n",
    "subset_a = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\"]\n",
    "subset_b = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\n",
    "            \"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\n",
    "            \"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\"]\n",
    "\n",
    "\n",
    "# SUBSET A\n",
    "print(\"RANDOM FOREST SUBSET A\")\n",
    "\n",
    "# model\n",
    "rf = xgb.XGBRegressor(objective='binary:logistic',\n",
    "                      eval_metric='error',\n",
    "                      seed=229,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "# parameters to try\n",
    "parameters = {\n",
    "    'n_estimators': (50,100,1000),\n",
    "    'max_depth': (2,4,6),\n",
    "    'learning_rate': (0.01, 0.1, 0.3)\n",
    "}\n",
    "\n",
    "# perform validation\n",
    "gs_rf = GridSearchCV(rf,\n",
    "                     parameters,\n",
    "                     cv=ShuffleSplit(n_splits=1,\n",
    "                                     test_size=0.13,\n",
    "                                     random_state=229))\n",
    "gs_rf.fit(X_train[subset_a], y_train)\n",
    "print(gs_rf.cv_results_)\n",
    "print(gs_rf.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_rf.predict(X_test[subset_a])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "print(gs_rf.best_estimator_.feature_importances_)\n",
    "xgb.plot_importance(gs_rf.best_estimator_)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_full_a.png\")\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nRANDOM FOREST SUBSET B\")\n",
    "\n",
    "# model, parameters to try, gridsearch defined above\n",
    "\n",
    "# perform validation\n",
    "gs_rf.fit(X_train[subset_b], y_train)\n",
    "print(gs_rf.cv_results_)\n",
    "print(gs_rf.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_rf.predict(X_test[subset_b])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "print(gs_rf.best_estimator_.feature_importances_)\n",
    "xgb.plot_importance(gs_rf.best_estimator_)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_full_b.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST SUBSET A\n",
      "{'mean_fit_time': array([  3.51418614,   6.85064244,  68.79172134,   6.88028097,\n",
      "        13.86282659, 137.44735312,  11.56536579,  22.55790138,\n",
      "       164.13793898,   1.70830536,   2.9335022 ,  30.44563985,\n",
      "         3.00464463,   5.72299266,  65.98100615,   5.41279936,\n",
      "        12.39310312, 104.2001369 ,   1.7332449 ,   2.89723396,\n",
      "        30.98867297,   3.05023742,   5.87534022,  63.52798462,\n",
      "         5.15996504,  10.70991874, 101.21425319]), 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'mean_score_time': array([0.01766348, 0.02018762, 0.03984666, 0.01812339, 0.02254891,\n",
      "       0.062222  , 0.0206213 , 0.0272181 , 0.08673191, 0.02199793,\n",
      "       0.02235198, 0.03135157, 0.01505351, 0.01955771, 0.06196165,\n",
      "       0.00937319, 0.01621628, 0.06879282, 0.00861931, 0.00882483,\n",
      "       0.02859402, 0.01159883, 0.01069736, 0.04366946, 0.01987934,\n",
      "       0.01682734, 0.08060813]), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 2, 2, 2, 4, 4, 4, 6, 6, 6,\n",
      "                   2, 2, 2, 4, 4, 4, 6, 6, 6],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50, 100,\n",
      "                   1000, 50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50,\n",
      "                   100, 1000, 50, 100, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 1000}], 'split0_test_score': array([0.07359445, 0.10244454, 0.13743587, 0.08072683, 0.11203459,\n",
      "       0.14839527, 0.08601377, 0.11938138, 0.15787495, 0.13266442,\n",
      "       0.13769842, 0.15568199, 0.142106  , 0.14883653, 0.17204754,\n",
      "       0.15252557, 0.15824334, 0.18083629, 0.14193925, 0.14650293,\n",
      "       0.16627655, 0.15278722, 0.15906315, 0.1804454 , 0.16334002,\n",
      "       0.17000519, 0.17915096]), 'mean_test_score': array([0.07359445, 0.10244454, 0.13743587, 0.08072683, 0.11203459,\n",
      "       0.14839527, 0.08601377, 0.11938138, 0.15787495, 0.13266442,\n",
      "       0.13769842, 0.15568199, 0.142106  , 0.14883653, 0.17204754,\n",
      "       0.15252557, 0.15824334, 0.18083629, 0.14193925, 0.14650293,\n",
      "       0.16627655, 0.15278722, 0.15906315, 0.1804454 , 0.16334002,\n",
      "       0.17000519, 0.17915096]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([27, 24, 20, 26, 23, 15, 25, 22, 10, 21, 19, 11, 17, 14,  4, 13,  9,\n",
      "        1, 18, 16,  6, 12,  8,  2,  7,  5,  3], dtype=int32)}\n",
      "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79    235680\n",
      "           1       0.36      0.64      0.46     61476\n",
      "\n",
      "    accuracy                           0.69    297156\n",
      "   macro avg       0.62      0.67      0.62    297156\n",
      "weighted avg       0.78      0.69      0.72    297156\n",
      "\n",
      "[[166636  69044]\n",
      " [ 22010  39466]]\n",
      "0.6745087762296654\n",
      "[0.5202494  0.08975926 0.29301322 0.09697807]\n",
      "\n",
      "\n",
      "RANDOM FOREST SUBSET B\n",
      "{'mean_fit_time': array([  3.00838661,   7.02139759,  54.64507461,   5.73264599,\n",
      "        12.89861321, 118.69419432,   9.4062202 ,  20.78481507,\n",
      "       193.15963149,   3.04959083,   5.55092001,  58.08820343,\n",
      "         6.56505632,  11.31047916, 111.39446306,  10.3372767 ,\n",
      "        20.07197165, 176.4908669 ,   3.20086288,   5.70069671,\n",
      "        58.2186296 ,   5.53252554,  12.585675  , 110.3556726 ,\n",
      "         8.53225899,  19.28074431, 181.34389567]), 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'mean_score_time': array([0.01641321, 0.01733804, 0.03823519, 0.01520753, 0.0200448 ,\n",
      "       0.05366778, 0.01563168, 0.03595209, 0.08745885, 0.01636219,\n",
      "       0.01543188, 0.04209948, 0.01693845, 0.02792621, 0.04955912,\n",
      "       0.03322816, 0.02158165, 0.07302475, 0.01127219, 0.01762366,\n",
      "       0.03169155, 0.01245046, 0.02027559, 0.05077887, 0.01251245,\n",
      "       0.02209973, 0.06891227]), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
      "                   0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_depth': masked_array(data=[2, 2, 2, 4, 4, 4, 6, 6, 6, 2, 2, 2, 4, 4, 4, 6, 6, 6,\n",
      "                   2, 2, 2, 4, 4, 4, 6, 6, 6],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50, 100,\n",
      "                   1000, 50, 100, 1000, 50, 100, 1000, 50, 100, 1000, 50,\n",
      "                   100, 1000, 50, 100, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 2, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 4, 'n_estimators': 1000}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 50}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 100}, {'learning_rate': 0.3, 'max_depth': 6, 'n_estimators': 1000}], 'split0_test_score': array([0.08085509, 0.11583106, 0.16708871, 0.09414341, 0.13174898,\n",
      "       0.17789556, 0.10119387, 0.14133749, 0.1862636 , 0.15935824,\n",
      "       0.16739807, 0.18453431, 0.17146918, 0.17799079, 0.19716479,\n",
      "       0.18055097, 0.18655655, 0.20300901, 0.16966031, 0.17569836,\n",
      "       0.19354304, 0.18136098, 0.18821024, 0.20143293, 0.18822436,\n",
      "       0.19235009, 0.19314861]), 'mean_test_score': array([0.08085509, 0.11583106, 0.16708871, 0.09414341, 0.13174898,\n",
      "       0.17789556, 0.10119387, 0.14133749, 0.1862636 , 0.15935824,\n",
      "       0.16739807, 0.18453431, 0.17146918, 0.17799079, 0.19716479,\n",
      "       0.18055097, 0.18655655, 0.20300901, 0.16966031, 0.17569836,\n",
      "       0.19354304, 0.18136098, 0.18821024, 0.20143293, 0.18822436,\n",
      "       0.19235009, 0.19314861]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([27, 24, 20, 26, 23, 15, 25, 22, 10, 21, 19, 11, 17, 14,  3, 13,  9,\n",
      "        1, 18, 16,  4, 12,  8,  2,  7,  6,  5], dtype=int32)}\n",
      "{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.70      0.78    235680\n",
      "           1       0.37      0.67      0.48     61476\n",
      "\n",
      "    accuracy                           0.70    297156\n",
      "   macro avg       0.63      0.69      0.63    297156\n",
      "weighted avg       0.78      0.70      0.72    297156\n",
      "\n",
      "[[165171  70509]\n",
      " [ 20100  41376]]\n",
      "0.6869352659306118\n",
      "[0.31623474 0.05476329 0.14871904 0.05560786 0.18008639 0.03305237\n",
      " 0.02635717 0.02938168 0.03148766 0.0243603  0.07605897 0.02389051]\n",
      "\n",
      "\n",
      "RANDOM FOREST BOW\n",
      "Fitting 1 folds for each of 8 candidates, totalling 8 fits\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=4, xgbregressor__n_estimators=100;, score=0.187 total time= 1.5min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=4, xgbregressor__n_estimators=1000;, score=0.218 total time= 5.7min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=6, xgbregressor__n_estimators=100;, score=0.198 total time= 1.8min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=6, xgbregressor__n_estimators=1000;, score=0.228 total time= 8.1min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=4, xgbregressor__n_estimators=100;, score=0.201 total time= 1.5min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=4, xgbregressor__n_estimators=1000;, score=0.228 total time= 5.6min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=6, xgbregressor__n_estimators=100;, score=0.209 total time= 1.8min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=6, xgbregressor__n_estimators=1000;, score=0.226 total time= 8.1min\n",
      "{'mean_fit_time': array([ 83.15421534, 334.77027106, 101.64556479, 480.21353316,\n",
      "        80.01000905, 328.93890905,  98.32298374, 480.56381607]), 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'mean_score_time': array([8.01058674, 7.87464738, 7.49460459, 7.76676965, 7.49518061,\n",
      "       7.85810041, 7.71226239, 7.87439394]), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'param_xgbregressor__learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_xgbregressor__max_depth': masked_array(data=[4, 4, 6, 6, 4, 4, 6, 6],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_xgbregressor__n_estimators': masked_array(data=[100, 1000, 100, 1000, 100, 1000, 100, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 1000}, {'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 1000}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 1000}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 1000}], 'split0_test_score': array([0.18738697, 0.21846862, 0.19780312, 0.22755832, 0.20076258,\n",
      "       0.22814232, 0.20947741, 0.2261683 ]), 'mean_test_score': array([0.18738697, 0.21846862, 0.19780312, 0.22755832, 0.20076258,\n",
      "       0.22814232, 0.20947741, 0.2261683 ]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([8, 4, 7, 2, 6, 1, 5, 3], dtype=int32)}\n",
      "{'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 1000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.72      0.80    235680\n",
      "           1       0.39      0.69      0.50     61476\n",
      "\n",
      "    accuracy                           0.71    297156\n",
      "   macro avg       0.64      0.70      0.65    297156\n",
      "weighted avg       0.79      0.71      0.74    297156\n",
      "\n",
      "[[168705  66975]\n",
      " [ 18881  42595]]\n",
      "0.7043471582604919\n",
      "['user_reviews' 'num_words' 'countvectorizer__katniss'\n",
      " 'countvectorizer__collection' 'countvectorizer__exchange'\n",
      " 'countvectorizer__short' 'countvectorizer__hype'\n",
      " 'countvectorizer__hunger' 'countvectorizer__clary'\n",
      " 'countvectorizer__potter' 'countvectorizer__men' 'countvectorizer__feyre'\n",
      " 'countvectorizer__maas' 'countvectorizer__rowell' 'countvectorizer__ya'\n",
      " 'countvectorizer__crime' 'countvectorizer__cinder'\n",
      " 'countvectorizer__earc' 'countvectorizer__colleen'\n",
      " 'countvectorizer__stiefvater' 'countvectorizer__hazel'\n",
      " 'countvectorizer__dystopian' 'countvectorizer__arc'\n",
      " 'countvectorizer__author' 'countvectorizer__tris'\n",
      " 'countvectorizer__thanks' 'countvectorizer__meyer'\n",
      " 'countvectorizer__faerie' 'countvectorizer__hero' 'countvectorizer__tm'\n",
      " 'countvectorizer__thank' 'countvectorizer__bardugo' 'quote' 'user_rating'\n",
      " 'countvectorizer__guilty' 'countvectorizer__grisha'\n",
      " 'countvectorizer__free' 'countvectorizer__hoover'\n",
      " 'countvectorizer__novella' 'countvectorizer__writer'\n",
      " 'countvectorizer__amazon' 'countvectorizer__anthology'\n",
      " 'countvectorizer__michael' 'countvectorizer__image'\n",
      " 'countvectorizer__fucking' 'countvectorizer__schwab'\n",
      " 'countvectorizer__br' 'countvectorizer__rowling'\n",
      " 'countvectorizer__originally' 'countvectorizer__watney']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00598376 0.00432864 0.00369671 0.00352712 0.00299867 0.00287864\n",
      " 0.00287486 0.00275257 0.00272919 0.00270093 0.00253641 0.00251157\n",
      " 0.00250328 0.00244664 0.00230591 0.00221026 0.00217029 0.00215959\n",
      " 0.00215363 0.00214862 0.00203982 0.00203303 0.00203055 0.00199099\n",
      " 0.00197456 0.00191422 0.00190005 0.00188784 0.00181462 0.00175\n",
      " 0.00166785 0.00160826 0.00160612 0.00157706 0.00149289 0.00148303\n",
      " 0.00145451 0.00145159 0.00145101 0.00143607 0.00143355 0.00141939\n",
      " 0.00141558 0.00140236 0.00138865 0.00138763 0.00137608 0.00137447\n",
      " 0.0013622  0.00134949]\n",
      "\n",
      "\n",
      "RANDOM FOREST TF-IDF\n",
      "Fitting 1 folds for each of 8 candidates, totalling 8 fits\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=4, xgbregressor__n_estimators=100;, score=0.188 total time= 2.8min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=4, xgbregressor__n_estimators=1000;, score=0.219 total time=17.6min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=6, xgbregressor__n_estimators=100;, score=0.198 total time= 3.7min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.1, xgbregressor__max_depth=6, xgbregressor__n_estimators=1000;, score=0.228 total time=26.0min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=4, xgbregressor__n_estimators=100;, score=0.200 total time= 2.8min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=4, xgbregressor__n_estimators=1000;, score=0.231 total time=17.6min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=6, xgbregressor__n_estimators=100;, score=0.210 total time= 3.7min\n",
      "[CV 1/1] END xgbregressor__learning_rate=0.3, xgbregressor__max_depth=6, xgbregressor__n_estimators=1000;, score=0.231 total time=25.8min\n",
      "{'mean_fit_time': array([ 159.05501819, 1047.99147391,  212.22962117, 1551.59969401,\n",
      "        157.2399745 , 1043.44380212,  210.88593793, 1537.53893781]), 'std_fit_time': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'mean_score_time': array([ 9.44689012,  9.49380207,  9.27237058,  9.89815569,  9.4509027 ,\n",
      "        9.84747601,  9.96191144, 10.14096689]), 'std_score_time': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'param_xgbregressor__learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_xgbregressor__max_depth': masked_array(data=[4, 4, 6, 6, 4, 4, 6, 6],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_xgbregressor__n_estimators': masked_array(data=[100, 1000, 100, 1000, 100, 1000, 100, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 1000}, {'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.1, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 1000}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 4, 'xgbregressor__n_estimators': 1000}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 100}, {'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 1000}], 'split0_test_score': array([0.18778766, 0.21917012, 0.19794289, 0.22788171, 0.2003833 ,\n",
      "       0.23050988, 0.20958685, 0.23084937]), 'mean_test_score': array([0.18778766, 0.21917012, 0.19794289, 0.22788171, 0.2003833 ,\n",
      "       0.23050988, 0.20958685, 0.23084937]), 'std_test_score': array([0., 0., 0., 0., 0., 0., 0., 0.]), 'rank_test_score': array([8, 4, 7, 3, 6, 2, 5, 1], dtype=int32)}\n",
      "{'xgbregressor__learning_rate': 0.3, 'xgbregressor__max_depth': 6, 'xgbregressor__n_estimators': 1000}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/rf_predictions.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d7b11a7b1d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_tf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/rf_predictions.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/rf_predictions.pkl'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "# feature subsets\n",
    "subset_a = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\"]\n",
    "subset_b = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\n",
    "            \"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\n",
    "            \"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\"]\n",
    "\n",
    "\n",
    "# SUBSET A\n",
    "print(\"RANDOM FOREST SUBSET A\")\n",
    "\n",
    "# model\n",
    "rf = xgb.XGBRegressor(objective='binary:logistic',\n",
    "                      eval_metric='error',\n",
    "                      seed=229,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "# parameters to try\n",
    "parameters = {\n",
    "    'n_estimators': (50,100,1000),\n",
    "    'max_depth': (2,4,6),\n",
    "    'learning_rate': (0.01, 0.1, 0.3)\n",
    "}\n",
    "\n",
    "# perform validation\n",
    "gs_rf = GridSearchCV(rf,\n",
    "                     parameters,\n",
    "                     cv=ShuffleSplit(n_splits=1,\n",
    "                                     test_size=0.13,\n",
    "                                     random_state=229))\n",
    "gs_rf.fit(X_train[subset_a], y_train)\n",
    "print(gs_rf.cv_results_)\n",
    "print(gs_rf.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_rf.predict(X_test[subset_a])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "print(gs_rf.best_estimator_.feature_importances_)\n",
    "xgb.plot_importance(gs_rf.best_estimator_)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_subseta.png\")\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nRANDOM FOREST SUBSET B\")\n",
    "\n",
    "# model, parameters to try, gridsearch defined above\n",
    "\n",
    "# perform validation\n",
    "gs_rf.fit(X_train[subset_b], y_train)\n",
    "print(gs_rf.cv_results_)\n",
    "print(gs_rf.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_rf.predict(X_test[subset_b])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "print(gs_rf.best_estimator_.feature_importances_)\n",
    "xgb.plot_importance(gs_rf.best_estimator_)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_subsetb.png\")\n",
    "\n",
    "# BAG OF WORDS\n",
    "print(\"\\n\\nRANDOM FOREST BOW\")\n",
    "\n",
    "# pipeline\n",
    "bow_pipe = make_pipeline(\n",
    "    ColumnTransformer(remainder='passthrough',\n",
    "                      transformers=[('countvectorizer',\n",
    "                                     CountVectorizer(max_features=10000),\n",
    "                                     'tokenized_words')]),\n",
    "    xgb.XGBRegressor(objective='binary:logistic',\n",
    "                     eval_metric='error',\n",
    "                     seed=229,\n",
    "                     n_jobs=-1))\n",
    "\n",
    "# parameters to try\n",
    "parameters = {\n",
    "    'xgbregressor__n_estimators': (100,1000),\n",
    "    'xgbregressor__max_depth': (4,6),\n",
    "    'xgbregressor__learning_rate': (0.1, 0.3)\n",
    "}\n",
    "\n",
    "# perform validation\n",
    "gs_bow_pipe = GridSearchCV(bow_pipe, \n",
    "                           parameters, \n",
    "                           cv=ShuffleSplit(n_splits=1, \n",
    "                                           test_size=0.13, \n",
    "                                           random_state=229),\n",
    "                           verbose=3)\n",
    "gs_bow_pipe.fit(X_train, y_train)\n",
    "print(gs_bow_pipe.cv_results_)\n",
    "print(gs_bow_pipe.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_bow_pipe.predict(X_test)\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "sorted_ind = gs_bow_pipe.best_estimator_.named_steps['xgbregressor'].feature_importances_.argsort()[::-1]\n",
    "print(np.take(gs_bow_pipe.best_estimator_.named_steps['columntransformer'].get_feature_names(),sorted_ind.tolist())[:50])\n",
    "print(np.take(gs_bow_pipe.best_estimator_.named_steps['xgbregressor'].feature_importances_,sorted_ind.tolist())[:50])\n",
    "xgb.plot_importance(gs_bow_pipe.best_estimator_.named_steps['xgbregressor'], max_num_features=50)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_bow.png\")\n",
    "\n",
    "# TF-IDF\n",
    "print(\"\\n\\nRANDOM FOREST TF-IDF\")\n",
    "\n",
    "# pipeline\n",
    "tf_pipe = make_pipeline(\n",
    "    ColumnTransformer(remainder='passthrough',\n",
    "                      transformers=[('tfidfvectorizer',\n",
    "                                     TfidfVectorizer(),\n",
    "                                     'tokenized_words')]),\n",
    "    xgb.XGBRegressor(objective='binary:logistic',\n",
    "                     eval_metric='error',\n",
    "                     seed=229,\n",
    "                     n_jobs=-1))\n",
    "\n",
    "# parameters to try\n",
    "parameters = {\n",
    "    'xgbregressor__n_estimators': (100,1000),\n",
    "    'xgbregressor__max_depth': (4,6),\n",
    "    'xgbregressor__learning_rate': (0.1, 0.3)\n",
    "}\n",
    "\n",
    "# perform validation\n",
    "gs_tf_pipe = GridSearchCV(tf_pipe, \n",
    "                           parameters, \n",
    "                           cv=ShuffleSplit(n_splits=1, \n",
    "                                           test_size=0.13, \n",
    "                                           random_state=229),\n",
    "                          verbose=3)\n",
    "gs_tf_pipe.fit(X_train, y_train)\n",
    "print(gs_tf_pipe.cv_results_)\n",
    "print(gs_tf_pipe.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = gs_tf_pipe.predict(X_test)\n",
    "with open(\"data/rf_predictions.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(predictions,fp)\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "sorted_ind = gs_tf_pipe.best_estimator_.named_steps['xgbregressor'].feature_importances_.argsort()[::-1]\n",
    "print(np.take(gs_tf_pipe.best_estimator_.named_steps['columntransformer'].get_feature_names(),sorted_ind.tolist())[:50])\n",
    "print(np.take(gs_tf_pipe.best_estimator_.named_steps['xgbregressor'].feature_importances_,sorted_ind.tolist())[:50])\n",
    "xgb.plot_importance(gs_tf_pipe.best_estimator_.named_steps['xgbregressor'], max_num_features=50)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_tfidf.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying new algos for better and faster results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Subset A: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88    235680\n",
      "           1       0.52      0.14      0.23     61476\n",
      "\n",
      "    accuracy                           0.80    297156\n",
      "   macro avg       0.67      0.55      0.55    297156\n",
      "weighted avg       0.75      0.80      0.75    297156\n",
      "\n",
      "[[227402   8278]\n",
      " [ 52582   8894]]\n",
      "0.5547752238251968\n",
      "\n",
      "\n",
      "Naive Bayes Subset B: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85    235680\n",
      "           1       0.41      0.34      0.37     61476\n",
      "\n",
      "    accuracy                           0.76    297156\n",
      "   macro avg       0.62      0.61      0.61    297156\n",
      "weighted avg       0.75      0.76      0.75    297156\n",
      "\n",
      "[[205908  29772]\n",
      " [ 40474  21002]]\n",
      "0.6076527123859639\n",
      "\n",
      "\n",
      "UNDERSAMPLE\n",
      "Naive Bayes Subset A: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86    235680\n",
      "           1       0.42      0.31      0.35     61476\n",
      "\n",
      "    accuracy                           0.77    297156\n",
      "   macro avg       0.62      0.60      0.61    297156\n",
      "weighted avg       0.75      0.77      0.75    297156\n",
      "\n",
      "[[209114  26566]\n",
      " [ 42554  18922]]\n",
      "0.5975371368410424\n",
      "\n",
      "\n",
      "Naive Bayes Subset B: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.69      0.77    235680\n",
      "           1       0.33      0.57      0.41     61476\n",
      "\n",
      "    accuracy                           0.67    297156\n",
      "   macro avg       0.59      0.63      0.59    297156\n",
      "weighted avg       0.75      0.67      0.69    297156\n",
      "\n",
      "[[163030  72650]\n",
      " [ 26360  35116]]\n",
      "0.631478912208417\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# feature subsets\n",
    "subset_a = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\"]\n",
    "subset_b = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\n",
    "            \"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\n",
    "            \"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\"]\n",
    "\n",
    "# SUBSET A\n",
    "print(\"Naive Bayes Subset A: \")\n",
    "\n",
    "# train\n",
    "nbTrainA = GaussianNB()\n",
    "nbTrainA.fit(X_train[subset_a], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainA.predict(X_test[subset_a])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nNaive Bayes Subset B: \")\n",
    "\n",
    "# train\n",
    "nbTrainB = GaussianNB()\n",
    "nbTrainB.fit(X_train[subset_b], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainB.predict(X_test[subset_b])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# UNDERSAMPLE\n",
    "print(\"\\n\\nUNDERSAMPLE\")\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "# SUBSET A\n",
    "print(\"Naive Bayes Subset A: \")\n",
    "\n",
    "# train\n",
    "nbTrainA = GaussianNB()\n",
    "nbTrainA.fit(X_train[subset_a], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainA.predict(X_test[subset_a])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nNaive Bayes Subset B: \")\n",
    "\n",
    "# train\n",
    "nbTrainB = GaussianNB()\n",
    "nbTrainB.fit(X_train[subset_b], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainB.predict(X_test[subset_b])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't use tf-idf here with GaussianNB as it converts the vectors into a sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Naive Bayes TF-IDF\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1bf1b0fea37e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mtf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \"\"\"\n\u001b[1;32m    242\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         return self._partial_fit(\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         )\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0m_ensure_no_complex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         array = _ensure_sparse_format(\n\u001b[0m\u001b[1;32m    819\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    509\u001b[0m             \u001b[0;34m\"A sparse matrix was passed, but dense \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;34m\"data is required. Use X.toarray() to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "# TF-IDF\n",
    "print(\"\\n\\nNaive Bayes TF-IDF\")\n",
    "\n",
    "# pipeline\n",
    "tf_pipe = make_pipeline(\n",
    "    ColumnTransformer(remainder='passthrough',\n",
    "                      transformers=[('tfidfvectorizer',\n",
    "                                     TfidfVectorizer(),\n",
    "                                     'tokenized_words')]),\n",
    "    StandardScaler(with_mean=False),\n",
    "    GaussianNB())\n",
    "\n",
    "# parameters to try\n",
    "#parameters = {\n",
    "#    'logisticregression__C': (10, 1, 0.01, 0.001)\n",
    "#}\n",
    "\n",
    "#perform validation\n",
    "gs_tf_pipe = GridSearchCV(tf_pipe, \n",
    "                           parameters, \n",
    "                           cv=ShuffleSplit(n_splits=1, \n",
    "                                           test_size=0.13, \n",
    "                                          random_state=229))\n",
    "\n",
    "\n",
    "tf_pipe.fit(X_train, y_train)\n",
    "print(tf_pipe.cv_results_)\n",
    "print(tf_pipe.best_params_)\n",
    "\n",
    "# predict\n",
    "predictions = tf_pipe.predict(X_test)\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# feature importance\n",
    "#coefficients = gs_tf_pipe.best_estimator_.named_steps['logisticregression'].coef_[0]\n",
    "#num_nonzero_coefs = len(np.where(abs(coefficients) > 0)[0])\n",
    "#sorted_ind = np.argsort(abs(coefficients))[::-1][:num_nonzero_coefs]\n",
    "#print(len(sorted_ind))\n",
    "#print(np.take(coefficients,sorted_ind.tolist()))\n",
    "#print(np.take(gs_tf_pipe.best_estimator_.named_steps['columntransformer'].get_feature_names(),sorted_ind.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can't use Multinomial NB for this as our X_train has some negative values.**\n",
    "\n",
    "Scale the dataset before using Multinomial or use tf-idf vectors instead of normal text vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Subset A: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-707d9814b520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mnbTrainA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mnbTrainA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubset_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# feature subsets\n",
    "subset_a = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\"]\n",
    "subset_b = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\n",
    "            \"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\n",
    "            \"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\"]\n",
    "\n",
    "# SUBSET A\n",
    "print(\"Naive Bayes Subset A: \")\n",
    "\n",
    "# train\n",
    "nbTrainA = MultinomialNB()\n",
    "nbTrainA.fit(X_train[subset_a], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainA.predict(X_test[subset_a])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nNaive Bayes Subset B: \")\n",
    "\n",
    "# train\n",
    "nbTrainB = MultinomialNB()\n",
    "nbTrainB.fit(X_train[subset_b], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainB.predict(X_test[subset_b])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# UNDERSAMPLE\n",
    "print(\"\\n\\nUNDERSAMPLE\")\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "# SUBSET A\n",
    "print(\"Naive Bayes Subset A: \")\n",
    "\n",
    "# train\n",
    "nbTrainA = GaussianNB()\n",
    "nbTrainA.fit(X_train[subset_a], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainA.predict(X_test[subset_a])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# SUBSET B\n",
    "print(\"\\n\\nNaive Bayes Subset B: \")\n",
    "\n",
    "# train\n",
    "nbTrainB = GaussianNB()\n",
    "nbTrainB.fit(X_train[subset_b], y_train)\n",
    "\n",
    "# predict\n",
    "predictions = nbTrainB.predict(X_test[subset_b])\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "Best known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;countvectorizer&#x27;,\n",
       "                                                  CountVectorizer(),\n",
       "                                                  &#x27;tokenized_words&#x27;)])),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;sgdclassifier&#x27;, SGDClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;countvectorizer&#x27;,\n",
       "                                                  CountVectorizer(),\n",
       "                                                  &#x27;tokenized_words&#x27;)])),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;sgdclassifier&#x27;, SGDClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;countvectorizer&#x27;, CountVectorizer(),\n",
       "                                 &#x27;tokenized_words&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>tokenized_words</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;user_reviews&#x27;, &#x27;days_since_review&#x27;, &#x27;user_rating&#x27;, &#x27;rating_diff&#x27;, &#x27;num_words&#x27;, &#x27;avg_word_len&#x27;, &#x27;avg_sent_len&#x27;, &#x27;pct_verbs&#x27;, &#x27;pct_nouns&#x27;, &#x27;pct_adj&#x27;, &#x27;quote&#x27;, &#x27;sentiment&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('countvectorizer',\n",
       "                                                  CountVectorizer(),\n",
       "                                                  'tokenized_words')])),\n",
       "                ('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('sgdclassifier', SGDClassifier())])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "print(\"Linear SVM: \")\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "sgd = make_pipeline(ColumnTransformer(remainder='passthrough',\n",
    "                      transformers=[('countvectorizer',\n",
    "                                     CountVectorizer(),\n",
    "                                     'tokenized_words')]),\n",
    "                    StandardScaler(with_mean= False), \n",
    "                    SGDClassifier(loss='hinge'))\n",
    "\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87    235680\n",
      "           1       0.44      0.29      0.35     61476\n",
      "\n",
      "    accuracy                           0.78    297156\n",
      "   macro avg       0.64      0.60      0.61    297156\n",
      "weighted avg       0.75      0.78      0.76    297156\n",
      "\n",
      "[[213091  22589]\n",
      " [ 43595  17881]]\n",
      "0.5975077059694713\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictions = sgd.predict(X_test)\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tfidf with Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;tfidfvectorizer&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;tokenized_words&#x27;)])),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;sgdclassifier&#x27;, SGDClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;tfidfvectorizer&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;tokenized_words&#x27;)])),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;sgdclassifier&#x27;, SGDClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;tfidfvectorizer&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;tokenized_words&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">tfidfvectorizer</label><div class=\"sk-toggleable__content\"><pre>tokenized_words</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;user_reviews&#x27;, &#x27;days_since_review&#x27;, &#x27;user_rating&#x27;, &#x27;rating_diff&#x27;, &#x27;num_words&#x27;, &#x27;avg_word_len&#x27;, &#x27;avg_sent_len&#x27;, &#x27;pct_verbs&#x27;, &#x27;pct_nouns&#x27;, &#x27;pct_adj&#x27;, &#x27;quote&#x27;, &#x27;sentiment&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('tfidfvectorizer',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'tokenized_words')])),\n",
       "                ('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('sgdclassifier', SGDClassifier())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "print(\"Linear SVM: \")\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "sgd = make_pipeline(ColumnTransformer(remainder='passthrough',\n",
    "                       transformers=[('tfidfvectorizer',\n",
    "                                     TfidfVectorizer(),\n",
    "                                     'tokenized_words')]),\n",
    "                    StandardScaler(with_mean= False), \n",
    "                    SGDClassifier(loss='hinge'))\n",
    "\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86    235680\n",
      "           1       0.42      0.30      0.35     61476\n",
      "\n",
      "    accuracy                           0.77    297156\n",
      "   macro avg       0.63      0.60      0.61    297156\n",
      "weighted avg       0.75      0.77      0.76    297156\n",
      "\n",
      "[[210210  25470]\n",
      " [ 42862  18614]]\n",
      "0.597357280916607\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "predictions = sgd.predict(X_test)\n",
    "predictions = list(map(round,predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "print(\"Linear SVM: \")\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "sgd = make_pipeline(ColumnTransformer(remainder='passthrough',\n",
    "                      transformers=[('countvectorizer',\n",
    "                                     CountVectorizer(),\n",
    "                                     'tokenized_words')]),\n",
    "                    StandardScaler(with_mean= False), \n",
    "                    SGDClassifier(loss='hinge'))\n",
    "\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Physical devices cannot be modified after being initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aecef28afcf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphysical_devices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/config.py\u001b[0m in \u001b[0;36mset_memory_growth\u001b[0;34m(device, enable)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRuntime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malready\u001b[0m \u001b[0minitialized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m   \"\"\"\n\u001b[0;32m--> 594\u001b[0;31m   \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mset_memory_growth\u001b[0;34m(self, dev, enable)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m       raise RuntimeError(\n\u001b[0m\u001b[1;32m   1449\u001b[0m           \"Physical devices cannot be modified after being initialized\")\n\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Physical devices cannot be modified after being initialized"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEURAL NET SUBSET A\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(1000, 4), b.shape=(4, 5), m=1000, n=5, k=4\n\t [[node sequential/dense/MatMul (defined at <ipython-input-1-77a09d2d572b>:56) ]] [Op:__inference_train_function_727]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-77a09d2d572b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# fit neural net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m history = mod.fit(x=X_train[subset_a],\n\u001b[0m\u001b[1;32m     57\u001b[0m                   \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(1000, 4), b.shape=(4, 5), m=1000, n=5, k=4\n\t [[node sequential/dense/MatMul (defined at <ipython-input-1-77a09d2d572b>:56) ]] [Op:__inference_train_function_727]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import shap\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# feature subsets\n",
    "subset_a = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\"]\n",
    "subset_b = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\n",
    "            \"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\n",
    "            \"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\"]\n",
    "\n",
    "\n",
    "# SUBSET A\n",
    "print(\"NEURAL NET SUBSET A\")\n",
    "\n",
    "# build neural net\n",
    "mod = Sequential()\n",
    "# input layer\n",
    "mod.add(Dense(units=5, input_dim=X_train[subset_a].shape[1], activation='relu'))\n",
    "# first hidden layer\n",
    "mod.add(Dense(units=4, activation='relu'))\n",
    "# output layer\n",
    "mod.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "mod.compile(loss='binary_crossentropy'\n",
    "            , optimizer='adam'\n",
    "            , metrics=['accuracy'])\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit neural net\n",
    "history = mod.fit(x=X_train[subset_a],\n",
    "                  y=y_train,\n",
    "                  validation_split=0.13,\n",
    "                  epochs=500,\n",
    "                  batch_size=1000,\n",
    "                  workers=-1,\n",
    "                  use_multiprocessing=True,\n",
    "                  verbose=0,\n",
    "                  callbacks=[es, mc])\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# plot validation accuracy per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_acc_a\")\n",
    "# plot loss per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_loss_a\")\n",
    "\n",
    "# predictions with saved weights from best validation accuracy\n",
    "mod.load_weights('best_model.h5')\n",
    "predictions = (mod.predict(X_test[subset_a]) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# Shapley Values\n",
    "explainer = shap.DeepExplainer(mod, np.array(X_train[subset_a][:5000]))\n",
    "shap_values = explainer.shap_values(np.array(X_test[subset_a][:1000]))\n",
    "\n",
    "# plot mean absolute value\n",
    "shap_df = pd.DataFrame(shap_values[0],columns=X_train[subset_a].columns)\n",
    "shap_abs_mean = shap_df.abs().mean().sort_values()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(shap_abs_mean.index, shap_abs_mean)\n",
    "plt.xlabel(\"mean |SHAP value|\")\n",
    "plt.grid(False,axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_ma_a\")\n",
    "\n",
    "# plot all values\n",
    "shap_df = shap_df.melt()\n",
    "shap_df[\"sign\"] = shap_df[\"value\"] > 0\n",
    "plt.figure(figsize=(5,10))\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] == True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] == True][\"variable\"],\n",
    "                   color=\"red\")\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] != True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] != True][\"variable\"],\n",
    "                   color=\"blue\")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_a\")\n",
    "\n",
    "\n",
    "# SUBSET B\n",
    "print(\"NEURAL NET SUBSET B\")\n",
    "\n",
    "# build neural net\n",
    "mod = Sequential()\n",
    "# input layer\n",
    "mod.add(Dense(units=13, input_dim=X_train[subset_b].shape[1], activation='relu'))\n",
    "# first hidden layer\n",
    "mod.add(Dense(units=9, activation='relu'))\n",
    "# output layer\n",
    "mod.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "mod.compile(loss='binary_crossentropy'\n",
    "            , optimizer='adam'\n",
    "            , metrics=['accuracy'])\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit neural net\n",
    "history = mod.fit(x=X_train[subset_b],\n",
    "                  y=y_train,\n",
    "                  validation_split=0.13,\n",
    "                  epochs=500,\n",
    "                  batch_size=1000,\n",
    "                  workers=-1,\n",
    "                  use_multiprocessing=True,\n",
    "                  verbose=0,\n",
    "                  callbacks=[es, mc])\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# plot validation accuracy per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_acc_b\")\n",
    "# plot loss per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_loss_b\")\n",
    "\n",
    "# predictions with saved weights from best validation accuracy\n",
    "mod.load_weights('best_model.h5')\n",
    "predictions = (mod.predict(X_test[subset_b]) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# Shapley Values\n",
    "explainer = shap.DeepExplainer(mod, np.array(X_train[subset_b][:5000]))\n",
    "shap_values = explainer.shap_values(np.array(X_test[subset_b][:1000]))\n",
    "\n",
    "# plot mean absolute value\n",
    "shap_df = pd.DataFrame(shap_values[0],columns=X_train[subset_b].columns)\n",
    "shap_abs_mean = shap_df.abs().mean().sort_values()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(shap_abs_mean.index, shap_abs_mean)\n",
    "plt.xlabel(\"mean |SHAP value|\")\n",
    "plt.grid(False,axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_ma_b\")\n",
    "\n",
    "# plot all values\n",
    "shap_df = shap_df.melt()\n",
    "shap_df[\"sign\"] = shap_df[\"value\"] > 0\n",
    "plt.figure(figsize=(5,10))\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] == True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] == True][\"variable\"],\n",
    "                   color=\"red\")\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] != True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] != True][\"variable\"],\n",
    "                   color=\"blue\")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import shap\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "# feature subsets\n",
    "subset_a = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\"]\n",
    "subset_b = [\"user_reviews\",\"days_since_review\",\"user_rating\",\"rating_diff\",\n",
    "            \"num_words\",\"avg_word_len\",\"avg_sent_len\",\"pct_verbs\",\n",
    "            \"pct_nouns\",\"pct_adj\",\"quote\",\"sentiment\"]\n",
    "\n",
    "\n",
    "# SUBSET A\n",
    "print(\"NEURAL NET SUBSET A\")\n",
    "\n",
    "# build neural net\n",
    "mod = Sequential()\n",
    "# input layer\n",
    "mod.add(Dense(units=5, input_dim=X_train[subset_a].shape[1], activation='relu'))\n",
    "# first hidden layer\n",
    "mod.add(Dense(units=4, activation='relu'))\n",
    "# output layer\n",
    "mod.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "mod.compile(loss='binary_crossentropy'\n",
    "            , optimizer='adam'\n",
    "            , metrics=['accuracy'])\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit neural net\n",
    "history = mod.fit(x=X_train[subset_a],\n",
    "                  y=y_train,\n",
    "                  validation_split=0.13,\n",
    "                  epochs=500,\n",
    "                  batch_size=1000,\n",
    "                  workers=-1,\n",
    "                  use_multiprocessing=True,\n",
    "                  verbose=0,\n",
    "                  callbacks=[es, mc])\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# plot validation accuracy per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_acc_a_u\")\n",
    "# plot loss per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_loss_a_u\")\n",
    "\n",
    "# predictions with saved weights from best validation accuracy\n",
    "mod.load_weights('best_model.h5')\n",
    "predictions = (mod.predict(X_test[subset_a]) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# Shapley Values\n",
    "explainer = shap.DeepExplainer(mod, np.array(X_train[subset_a][:5000]))\n",
    "shap_values = explainer.shap_values(np.array(X_test[subset_a][:1000]))\n",
    "\n",
    "# plot mean absolute value\n",
    "shap_df = pd.DataFrame(shap_values[0],columns=X_train[subset_a].columns)\n",
    "shap_abs_mean = shap_df.abs().mean().sort_values()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(shap_abs_mean.index, shap_abs_mean)\n",
    "plt.xlabel(\"mean |SHAP value|\")\n",
    "plt.grid(False,axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_ma_a_u\")\n",
    "\n",
    "# plot all values\n",
    "shap_df = shap_df.melt()\n",
    "shap_df[\"sign\"] = shap_df[\"value\"] > 0\n",
    "plt.figure(figsize=(5,10))\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] == True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] == True][\"variable\"],\n",
    "                   color=\"red\")\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] != True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] != True][\"variable\"],\n",
    "                   color=\"blue\")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_a_u\")\n",
    "\n",
    "\n",
    "# SUBSET B\n",
    "print(\"NEURAL NET SUBSET B\")\n",
    "\n",
    "# build neural net\n",
    "mod = Sequential()\n",
    "# input layer\n",
    "mod.add(Dense(units=13, input_dim=X_train[subset_b].shape[1], activation='relu'))\n",
    "# first hidden layer\n",
    "mod.add(Dense(units=9, activation='relu'))\n",
    "# output layer\n",
    "mod.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "mod.compile(loss='binary_crossentropy'\n",
    "            , optimizer='adam'\n",
    "            , metrics=['accuracy'])\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit neural net\n",
    "history = mod.fit(x=X_train[subset_b],\n",
    "                  y=y_train,\n",
    "                  validation_split=0.13,\n",
    "                  epochs=500,\n",
    "                  batch_size=1000,\n",
    "                  workers=-1,\n",
    "                  use_multiprocessing=True,\n",
    "                  verbose=0,\n",
    "                  callbacks=[es, mc])\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# plot validation accuracy per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_acc_b_u\")\n",
    "# plot loss per epoch\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_loss_b_u\")\n",
    "\n",
    "# predictions with saved weights from best validation accuracy\n",
    "mod.load_weights('best_model.h5')\n",
    "predictions = (mod.predict(X_test[subset_b]) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# Shapley Values\n",
    "explainer = shap.DeepExplainer(mod, np.array(X_train[subset_b][:5000]))\n",
    "shap_values = explainer.shap_values(np.array(X_test[subset_b][:1000]))\n",
    "\n",
    "# plot mean absolute value\n",
    "shap_df = pd.DataFrame(shap_values[0],columns=X_train[subset_b].columns)\n",
    "shap_abs_mean = shap_df.abs().mean().sort_values()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(shap_abs_mean.index, shap_abs_mean)\n",
    "plt.xlabel(\"mean |SHAP value|\")\n",
    "plt.grid(False,axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_ma_b_u\")\n",
    "\n",
    "# plot all values\n",
    "shap_df = shap_df.melt()\n",
    "shap_df[\"sign\"] = shap_df[\"value\"] > 0\n",
    "plt.figure(figsize=(5,10))\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] == True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] == True][\"variable\"],\n",
    "                   color=\"red\")\n",
    "ax = sns.stripplot(x=shap_df[shap_df[\"sign\"] != True][\"value\"],\n",
    "                   y=shap_df[shap_df[\"sign\"] != True][\"variable\"],\n",
    "                   color=\"blue\")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_b_u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read/prep data\n",
    "dat = pd.read_csv(\"data/tokenized_reviews.csv\")\n",
    "dat = dat.dropna()\n",
    "dat[\"quote\"] = dat[\"quote\"].astype(int)\n",
    "dat[\"tokenized_words\"] = dat[\"tokenized_words\"].apply(lambda x: x.strip(\"[']\").replace(\"', '\",\" \"))\n",
    "\n",
    "# 85% train / 15% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat.drop(columns=[\"popular\"]), \n",
    "                                                    dat[\"popular\"],\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 229)\n",
    "\n",
    "# undersample train set\n",
    "majority_size = len(y_train[y_train==0])\n",
    "minority_size = len(y_train[y_train==1])\n",
    "majority_indices = y_train[y_train==0].index\n",
    "rng = np.random.default_rng(seed=229)\n",
    "drop_indices = rng.choice(majority_indices, majority_size-minority_size, replace=False)\n",
    "X_train = X_train.drop(drop_indices)\n",
    "y_train = y_train.drop(drop_indices)\n",
    "\n",
    "\n",
    "X_train.to_pickle(\"data/X_train.pkl\",protocol=3)\n",
    "X_test.to_pickle(\"data/X_test.pkl\",protocol=3)\n",
    "y_train.to_pickle(\"data/y_train.pkl\",protocol=3)\n",
    "y_test.to_pickle(\"data/y_test.pkl\",protocol=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import shap\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# read data\n",
    "X_train = pd.read_pickle(\"data/X_train.pkl\")\n",
    "X_test = pd.read_pickle(\"data/X_test.pkl\")\n",
    "y_train = pd.read_pickle(\"data/y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"data/y_test.pkl\")\n",
    "\n",
    "# BOW\n",
    "print(\"NEURAL NET BOW\")\n",
    "\n",
    "# build bag of words\n",
    "bow = ColumnTransformer(remainder='passthrough',\n",
    "                        transformers=[('countvectorizer',\n",
    "                                       CountVectorizer(max_features=10000),\n",
    "                                       'tokenized_words')])\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)\n",
    "\n",
    "X_train_bow, X_valid_bow, y_train, y_valid = train_test_split(X_train_bow,\n",
    "                                                              y_train,\n",
    "                                                              test_size = 0.13,\n",
    "                                                              random_state = 229)\n",
    "\n",
    "# build neural net\n",
    "mod = Sequential()\n",
    "# input layer\n",
    "mod.add(Dense(units=10013, input_dim=X_train_bow.shape[1], activation='relu'))\n",
    "# first hidden layer\n",
    "mod.add(Dense(units=6676, activation='relu'))\n",
    "# output layer\n",
    "mod.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "mod.compile(loss='binary_crossentropy'\n",
    "            , optimizer='adam'\n",
    "            , metrics=['accuracy'])\n",
    "\n",
    "# build generator (sparse too big to convert to dense all at once)\n",
    "batch_size = 1000\n",
    "epochs = 500\n",
    "samples_per_epoch = X_train_bow.shape[0]\n",
    "batches_per_epoch = samples_per_epoch//batch_size\n",
    "\n",
    "# https://stackoverflow.com/questions/37609892/keras-sparse-matrix-issue\n",
    "def batch_generator(X_train, y_train, batch_size, batches_per_epoch):\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y_train)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    X =  X_train[shuffle_index, :]\n",
    "    y =  y_train.to_numpy()[shuffle_index]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch,:].toarray()\n",
    "        y_batch = y[index_batch]\n",
    "        counter += 1\n",
    "        yield(X_batch,y_batch)\n",
    "        if (counter == batches_per_epoch):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter = 0\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit neural net\n",
    "history = mod.fit(x=batch_generator(X_train_bow, y_train, batch_size, batches_per_epoch),\n",
    "                  validation_data=(X_valid_bow.toarray(), y_valid),\n",
    "                  epochs=500,\n",
    "                  steps_per_epoch=batches_per_epoch,\n",
    "                  workers=-1,\n",
    "                  use_multiprocessing=True,\n",
    "                  verbose=1,\n",
    "                  callbacks=[es, mc])\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# plot validation accuracy per epoch\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"net_acc_bow\")\n",
    "# plot loss per epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"net_loss_bow\")\n",
    "\n",
    "# predictions with saved weights from best validation accuracy\n",
    "mod.load_weights('best_model.h5')\n",
    "predictions = (mod.predict(X_test_bow) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# Shapley Values\n",
    "explainer = shap.DeepExplainer(mod, X_train_bow[:5000].toarray())\n",
    "shap_values = explainer.shap_values(X_test_bow[:1000].toarray())\n",
    "\n",
    "# plot mean absolute value\n",
    "shap_df = pd.DataFrame(shap_values[0],columns=bow.get_feature_names())\n",
    "shap_abs_mean = shap_df.abs().mean().sort_values()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(shap_abs_mean[::-1][:25].index, shap_abs_mean[::-1][:25])\n",
    "plt.xlabel(\"mean |SHAP value|\")\n",
    "plt.grid(False,axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_ma_bow\")\n",
    "\n",
    "# plot all values\n",
    "shap_df = shap_df.melt()\n",
    "shap_df[\"sign\"] = shap_df[\"value\"] > 0\n",
    "shap_df_plot = shap_df[shap_df[\"variable\"].isin(shap_abs_mean[::-1][:25].index)]\n",
    "plt.figure(figsize=(5,10))\n",
    "ax = sns.stripplot(x=shap_df_plot[shap_df_plot[\"sign\"] == True][\"value\"],\n",
    "                   y=shap_df_plot[shap_df_plot[\"sign\"] == True][\"variable\"],\n",
    "                   color=\"red\")\n",
    "ax = sns.stripplot(x=shap_df_plot[shap_df_plot[\"sign\"] != True][\"value\"],\n",
    "                   y=shap_df_plot[shap_df_plot[\"sign\"] != True][\"variable\"],\n",
    "                   color=\"blue\")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_bow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import shap\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# read data\n",
    "X_train = pd.read_pickle(\"data/X_train.pkl\")\n",
    "X_test = pd.read_pickle(\"data/X_test.pkl\")\n",
    "y_train = pd.read_pickle(\"data/y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"data/y_test.pkl\")\n",
    "\n",
    "# TF-IDF\n",
    "print(\"NEURAL NET TF-IDF\")\n",
    "\n",
    "# build tfidf\n",
    "tf = ColumnTransformer(remainder='passthrough',\n",
    "                       transformers=[('tfidfvectorizer',\n",
    "                                      TfidfVectorizer(min_df=0.0001),\n",
    "                                      'tokenized_words')])\n",
    "X_train_tf = tf.fit_transform(X_train)\n",
    "X_test_tf = tf.transform(X_test)\n",
    "\n",
    "X_train_tf, X_valid_tf, y_train, y_valid = train_test_split(X_train_tf,\n",
    "                                                            y_train,\n",
    "                                                            test_size = 0.13,\n",
    "                                                            random_state = 229)\n",
    "\n",
    "# build neural net\n",
    "mod = Sequential()\n",
    "# input layer\n",
    "mod.add(Dense(units=26115, input_dim=X_train_tf.shape[1], activation='relu'))\n",
    "# first hidden layer\n",
    "mod.add(Dense(units=17411, activation='relu'))\n",
    "# output layer\n",
    "mod.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "mod.compile(loss='binary_crossentropy'\n",
    "            , optimizer='adam'\n",
    "            , metrics=['accuracy'])\n",
    "\n",
    "# build generator (sparse too big to convert to dense all at once)\n",
    "batch_size = 1000\n",
    "epochs = 500\n",
    "samples_per_epoch = X_train_tf.shape[0]\n",
    "batches_per_epoch = samples_per_epoch//batch_size\n",
    "\n",
    "# https://stackoverflow.com/questions/37609892/keras-sparse-matrix-issue\n",
    "def batch_generator(X_train, y_train, batch_size, batches_per_epoch):\n",
    "    counter=0\n",
    "    shuffle_index = np.arange(np.shape(y_train)[0])\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    X =  X_train[shuffle_index, :]\n",
    "    y =  y_train.to_numpy()[shuffle_index]\n",
    "    while 1:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[index_batch,:].toarray()\n",
    "        y_batch = y[index_batch]\n",
    "        counter += 1\n",
    "        yield(X_batch,y_batch)\n",
    "        if (counter == batches_per_epoch):\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            counter = 0\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "# fit neural net\n",
    "history = mod.fit(x=batch_generator(X_train_tf, y_train, batch_size, batches_per_epoch),\n",
    "                  validation_data=(X_valid_tf.toarray(), y_valid),\n",
    "                  epochs=500,\n",
    "                  steps_per_epoch=batches_per_epoch,\n",
    "                  workers=-1,\n",
    "                  use_multiprocessing=True,\n",
    "                  verbose=1,\n",
    "                  callbacks=[es, mc])\n",
    "\n",
    "# https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "# plot validation accuracy per epoch\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"net_acc_tf\")\n",
    "# plot loss per epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig(\"net_loss_tf\")\n",
    "\n",
    "# predictions with saved weights from best validation accuracy\n",
    "mod.load_weights('best_model.h5')\n",
    "predictions = (mod.predict(X_test_tf) > 0.5).astype(\"int32\")\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n",
    "# Shapley Values\n",
    "explainer = shap.DeepExplainer(mod, X_train_tf[:5000].toarray())\n",
    "shap_values = explainer.shap_values(X_test_tf[:1000].toarray())\n",
    "\n",
    "# plot mean absolute value\n",
    "shap_df = pd.DataFrame(shap_values[0],columns=tf.get_feature_names())\n",
    "shap_abs_mean = shap_df.abs().mean().sort_values()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(shap_abs_mean[::-1][:25].index, shap_abs_mean[::-1][:25])\n",
    "plt.xlabel(\"mean |SHAP value|\")\n",
    "plt.grid(False,axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_ma_tf\")\n",
    "\n",
    "# plot all values\n",
    "shap_df = shap_df.melt()\n",
    "shap_df[\"sign\"] = shap_df[\"value\"] > 0\n",
    "shap_df_plot = shap_df[shap_df[\"variable\"].isin(shap_abs_mean[::-1][:25].index)]\n",
    "plt.figure(figsize=(5,10))\n",
    "ax = sns.stripplot(x=shap_df_plot[shap_df_plot[\"sign\"] == True][\"value\"],\n",
    "                   y=shap_df_plot[shap_df_plot[\"sign\"] == True][\"variable\"],\n",
    "                   color=\"red\")\n",
    "ax = sns.stripplot(x=shap_df_plot[shap_df_plot[\"sign\"] != True][\"value\"],\n",
    "                   y=shap_df_plot[shap_df_plot[\"sign\"] != True][\"variable\"],\n",
    "                   color=\"blue\")\n",
    "plt.xlabel(\"SHAP value\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"net_shap_tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
